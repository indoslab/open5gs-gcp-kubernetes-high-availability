# Bootstrapping the Kubernetes Control Plane

## Only Controller-0 node
## SSH to controller-0 Nodes
```
gcloud compute ssh controller-0
```


## Create kubeadm config
“controlPlaneEndpoint” from External IP created in Kubernetes Public IP Address
```
cat > kubeadm-config.yaml <<EOF
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
kubernetesVersion: stable
controlPlaneEndpoint: "34.133.82.37:6443"
networking:
    podSubnet: "10.244.0.0/16"
EOF
```
                                
## Initialize the Cluster
```
sudo kubeadm init --config=kubeadm-config.yaml --upload-certs
```

## Initialize Result
```
You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/
You can now join any number of the control-plane node running the following command on each as root:
kubeadm join 34.133.82.37:6443 --token cnxnl7.fjmtxweii0jixgzg \
        --discovery-token-ca-cert-hash sha256:b68edd7cbd40cea2e29d123b189cd4582dc01be54600f52fdb3f1902dab5f613 \
        --control-plane --certificate-key e292fd85bc293ff95f6139d9a6a0d8b189b82856d9619613f4000ad2763ad77a
Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use
"kubeadm init phase upload-certs --upload-certs" to reload certs afterward.
Then you can join any number of worker nodes by running the following on each as root:
kubeadm join 34.133.82.37:6443 --token cnxnl7.fjmtxweii0jixgzg \
        --discovery-token-ca-cert-hash sha256:b68edd7cbd40cea2e29d123b189cd4582dc01be54600f52fdb3f1902dab5f613
```

                                
## Make Current user able to use kube commands
```
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```
                                
## Install Calico Networking (CNI)
                                
```
kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml
kubectl get pods -n kube-system
```

## exit
```                                
exit
```
                                
## All Master nodes except controller-0
## SSH to controller-1 and controller-2 Nodes
```
gcloud compute ssh controller-1
gcloud compute ssh controller-2
```
                                
## Join master nodes
```                                
sudo kubeadm join 34.133.82.37:6443 --token cnxnl7.fjmtxweii0jixgzg \
        --discovery-token-ca-cert-hash sha256:b68edd7cbd40cea2e29d123b189cd4582dc01be54600f52fdb3f1902dab5f613 \
        --control-plane --certificate-key e292fd85bc293ff95f6139d9a6a0d8b189b82856d9619613f4000ad2763ad77a
```

## Make Current user able to use kube commands
```                                
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

## exit                                
SSH to controller-0 Nodes
```
gcloud compute ssh controller-0
```

## Verify All master nodes already join
```
kubectl get nodes
NAME           STATUS   ROLES                  AGE     VERSION
controller-0   Ready    control-plane,master   5m21s   v1.22.1
controller-1   Ready    control-plane,master   2m2s    v1.22.1
controller-2   Ready    control-plane,master   32s     v1.22.1
```

## exit
```
exit
```
                                
## Add controller-1 and controller-2 in target pool Load Balancer from gcloud cli
```                                
gcloud compute target-pools add-instances kubernetes-target-pool \
   --instances controller-1,controller-2 \
   --instances-zone us-central1-c
```

Next: [Part 7 - Bootstrapping the Kubernetes Worker Nodes](08-part-07.md)
